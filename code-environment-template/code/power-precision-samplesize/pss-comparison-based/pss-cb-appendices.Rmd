---
title: "Project: [insert project name or codename]: Power and Sample Size Calculations based on Comparisons"
author: "[Eric J. Daza, DrPH, MPS | ericjdaza.com](https://www.ericjdaza.com/)"
date: "`r Sys.Date()`"
output:
  pdf_document:
  toc: true
---




# Appendix ((optional))



## Conceptual Review ((optional but recommended))

Statistical power and margin-of-error analyses are used to determine whether a sample may be “adequate or large enough” to discern a true, unknown quantity using a quantity calculated from an observable study sample.


### ((Text for Confirmatory Studies))

In a confirmatory study, we seek to test pre-planned or “a priori” scientific hypotheses. The burden of proof is to show that the hypothesis might be true, compared to the “baseline” truth in which it is false (i.e., a “null” hypothesis). The relevant calculations involve statistical power or precision, and try to ensure that study findings have a good chance of being statistically discernible, defined as having statistically significant p-values when compared to a null truth (which may be contrary to the actual but unknown true quantity).


### ((Text for Exploratory Studies))

In an exploratory study, we seek to characterize largely unknown relationships between ill-defined variables; i.e., generate scientific hypotheses to be tested in follow-up studies. While some calculations involve comparisons (as in confirmatory studies), many if not most do not involve comparisons because there are few hypotheses to confirm.

((optional)) We should still propose a priori hypotheses whenever possible in exploratory studies. A priori hypotheses will be the most generalizable because they will not rely on our data (i.e., they are created before seeing any study data). Hypotheses created using our data in the course of the study cannot use that same dataset to generalize them. To do so is called “hypothesizing after the results are known” (HARKing), a common precursor to p-hacking. HARKing is akin to overfitting in machine learning, and leads to over-confident conclusions that our study findings reflect true quantities (i.e., generalize).


### Making Comparisons: Power Analysis ((include if used))

A power analysis is used to characterize the relationship between these six core interdependent components:
  
1. differences in mean outcomes of interest across two or more categorical levels of another variable (e.g., effect sizes, association magnitudes)
1. number of statistical hypothesis tests to be conducted (e.g., research objectives)
1. outcome standard deviations (SDs)
1. outcome distributions (e.g., normal, binomial)
1. statistical properties (i.e., maximum false positive rates (FPRs) or statistical significance levels like $\alpha = 0.05$, statistical power values like $1 - \beta = 0.8$)
1. sample sizes

A power analysis must characterize the possible scenarios given pre-specified components. The more components are specified, the fewer the scenarios it must characterize.
For two-sample tests, note that Cohen’s $d$ for equal group sizes is equal to a difference in mean outcomes (for a given outcome) divided by the outcome SD.

#### ((Text for Confirmatory Studies))

In most simple confirmatory studies, only one of these components is unspecified; e.g., for one test with a specified difference, set of outcome SDs, normal distributions, maximum FPR of 0.01, and statistical power of 85%, we can calculate the corresponding sample size. Power analyses are statistically useful in these cases because they (in tandem with the SAP) allow a more detailed statistical characterization of scientific conclusions.

#### ((Text for Exploratory Studies))

In an exploratory study, we often cannot reasonably specify components 1-4, while we can specify 5 and 6 through tradition or common practice (e.g., maximum FPR level of 0.05, power of 0.80), and budgetary, logistical, or study feasibility constraints (e.g., sample size of 200). Hence, a reasonable power analysis would have to characterize a multitude of scenarios corresponding to different numbers of tests, different mean outcomes, different outcome SDs, and different outcome distributions.

Such results cannot be used to make a solid recommendation to pick from a few design options (as is common in confirmatory studies where most components are known). However, they can be used to strengthen the current study’s findings, which can be used to provide power analysis components for a future confirmatory study.


### Establishing Precision: Margin of Error Analysis ((include if used))

For hypotheses that are not stated as comparisons to a null, statistical power or statistical significance is undefined. Instead, the relevant calculations try to ensure that study findings have a good chance of being statistically discernible, defined as having a certain degree of estimator precision. For example, for an outcome with a given standard deviation, we can calculate the sample size needed to ensure a particular margin of error (MOE) corresponding to a certain size of confidence interval (CI).

The MOE formula is $MOE = z_{1-\alpha/2} \sigma/\sqrt{n}$, where $z_{1-\alpha/2}$ is the z-score corresponding to confidence level $1 - \alpha/2$ (e.g., 0.975 for a 95% CI),  is the outcome standard deviation (SD), and $n$ is the sample size. Hence, a MOE analysis is used to characterize the relationship between these six core interdependent components:
  
1. outcome SD
1. outcome distribution (e.g., normal, binomial)
1. statistical properties (i.e., CI or MOE and confidence level)
1. sample size

An MOE analysis must characterize the possible scenarios given pre-specified components. The more components are specified, the fewer the scenarios it must characterize.

#### ((Text for Confirmatory Studies))

In most confirmatory studies (like this one), only one of these components is unspecified; e.g., for one normally distributed outcome with a known SD and desired 95% confidence interval (CI) or MOE, we can calculate the corresponding sample size. MOE analyses are statistically useful in these cases because they (in tandem with the SAP) allow a more detailed statistical characterization of scientific conclusions.

#### ((Text for Exploratory Studies))

In an exploratory study (like this one), we often cannot reasonably specify components 1 and 2, while we can specify 3 and 4 through tradition or common practice (e.g., 95% CI of ± 0.1), and budgetary, logistical, or study feasibility constraints (e.g., sample size of 200). Hence, a reasonable MOE analysis would have to characterize a multitude of scenarios corresponding to different outcome SDs and distributions.

Such results cannot be used to make a solid recommendation to pick from a few design options (as is common in confirmatory studies where most components are known). However, they can be used to strengthen the current study’s findings, which can be used to provide power analysis components for a future confirmatory study.


### Accounting for Attrition (WIP)



## Methods


### Basic Setup and Parameters

The sample size required, $n$, depends on the following two quantities.

1. What is the highest chance of accidentally discerning/detecting a false average difference of size $\delta$ or larger, that we will allow or tolerate? This is the maximum false positive or "Type 1 error" rate we want, formally written as $\alpha$. The false positive rate (FPR) is defined as the probability of discerning/detecting a false average difference of this size by chance.

1. For a given $\alpha$, what is the lowest chance of discerning/detecting a true average difference of size $\delta$ or larger, that we will allow or tolerate? This is the statistical power we want, formally written as $1 - \beta$. Statistical power is defined as the probability of discerning/detecting a true average difference of this size for a given FPR. Put differently, it is the power to statistically discern/detect a true average difference of this size for a given FPR.

Note that $\alpha$ also equals our desired threshold for declaring a test's p-value to be statistically significant. That is, we decide beforehand that if a test's p-value is no larger than $\alpha$, then we can be confident that our sample average difference is an estimate of a true, unknown average difference that is not zero. How confident? $(1 - \alpha) \times 100\%$, which for $\alpha = 0.05$ equals 95%. We can now see how this $\alpha$ exactly corresponds to the degree of confidence expressed in a confidence interval.

In formal deliverables and manuscripts, we will avoid using the phrase "statistical significance" except when first defining the alternate, correct term to use. Instead, we will use the phrase "statistically discernible" or "statistically supported". (See **Appendix: Why and How to Drop the Phrase "Statistical Significance"** for motivation, details, and resources/references we will cite.) We will use the phrase "statistical significance" only if absolutely required by the client or reviewing body/organization.

Furthermore, if our estimation method or "estimator" is statistically consistent (which standard estimators are), then we can further conclude that a given estimate is unbiased for the true average difference with a large enough sample. This hypothesized true average difference is exactly what we use to determine the statistical power we want.


### Adjustment Parameters

#### Number of Tests

There is often more than one pre-planned statistical hypothesis test. The number of such tests corresponds with the number of research hypothesis findings that we would like to claim generalize beyond our study; i.e., generalizable findings are reproducible, meaning they can be approximately reproduced by another similar study with a different sample. These are called *a priori* or *ex ante* hypotheses because we document them before collecting or analyzing any study data. Analysis findings from a priori hypotheses are the most generalizable, compared to hypotheses generated or analysis adjusted after starting to collect or analyze data. See **Appendix: Confirmatory vs. Exploratory Objectives** for why.

The desired generalizable findings often correspond to a study's main research hypotheses or endpoints. If pre-study (i.e., a priori, ex ante) research interests include a more elaborate linearized model, sometimes researchers want to claim that the estimated coefficients of certain model terms generalize. The number of terms in such models must be included in the total number of tests to run.

For $m$ tests, we must adjust the FPR for each test in a way that preserves the overall or familywise $\alpha$ (as in the "family" of $m$ tests). This is called *multiplicity adjustment*. The easiest such adjustment is a simple *Bonferroni correction*, which equally splits the familywise $\alpha$ in $m$; i.e., each test's FPR is equal to $\alpha / m$.

#### Anticipated Attrition

Some participants may drop out of the study before the end of the study period. We say they "dropped out" or the study, or are "lost to follow-up", a process also called study sample attrition.

We will need to enroll a larger sample than needed to account for the maximum amount of anticipated attrition. This may occur, for example, due to non-engagement, non-adherence, or non-compliance with the treatment.

Let $p_{att}$ represent the anticipated proportion of attrition. The minimum larger sample needed to accommodate $p_{att} \times 100$% attrition is equal to $n_{adj} = n / (1 - p_{att})$.



## Repeated Measures


### T-Statistic: Outcome Variance


#### Two Groups

Let $X_i = 1$ if participant $i$ is assigned to active treatment, and let $X_i = 0$ if they're assigned to placebo control. For either treatment $x \in {0, 1}$, let $Y_{x,i}$ denote the pre-post difference in outcomes for participant $i$. Let $\mu_{Y(x)} = E(Y_i | X_i=x)$ denote the mean pre-post difference for treatment $x$, which we assume is the same for all participants receiving $x$.

We want to estimate $\delta = \mu_{Y(1)} - \mu_{Y(0)}$. The empirical average of the pre-post differences for each treatment is $\hat\mu_{Y(x)} = \frac{1}{n_x} \sum_{i=1}^{n_x} Y_{x,i}$. Let $\hat\delta = \hat\mu_{Y(1)} - \hat\mu_{Y(0)}$ denote our $\delta$ estimator.

The t-statistic we need to do our PSS calculations requires specifying both $E \big( \hat\delta \big) = \delta$ and $Var \big( \hat\delta \big)$. We assume the pre-post difference has the same variance no matter the treatment; i.e., we assume $Var \left( Y_{1,i} \right) = Var \left( Y_{0,i} \right) = Var(Y)$ for any participant $i$.

Note that we actually specify two $\delta$'s for PSS calculations; the null hypothesis $\delta_{null} = 0$ and the alternative hypothesis $\delta_{alt} \ne 0$, which we set to our effect size of interest. We will assume $Var \left( \hat\delta_{null} \right) = Var \left( \hat\delta_{alt} \right) = Var \big( \hat\delta \big)$.

We have:
  \[
    \begin{aligned}
    Var \big( \hat\delta \big)
    & = Var \left( \hat\mu_{Y(1)} - \hat\mu_{Y(0)} \right) \\
    & = Var \left(
      \frac{1}{n_1} \sum_{i=1}^{n_1} Y_{1,i} -
        \frac{1}{n_0} \sum_{i=1}^{n_0} Y_{0,i}
      \right) \\
    & = \frac{1}{n_1} Var \left( Y_{1,i} \right) + \frac{1}{n_0} Var \left( Y_{0,i} \right) \\
    & = \left( \frac{1}{n_1} + \frac{1}{n_0} \right) Var(Y)
    \end{aligned}
    \]


#### One Group

We want to estimate $\delta = \mu_{Y(1)} \equiv \mu_Y$. The empirical average of the pre-post differences is $\hat\delta = \hat\mu_Y$.

We have:
  \[
    \begin{aligned}
    Var \big( \hat\delta \big)
    & = Var \left( \hat\mu_Y \right) \\
    & = Var \left(
      \frac{1}{n_1} \sum_{i=1}^{n_1} Y_i
      \right) \\
    & = \frac{1}{n_1} Var \left( Y_i \right) \\
    & = \frac{1}{n_1} Var(Y)
    \end{aligned}
    \]


### Intraclass Correlation

The t-statistic variance formula has been straightforward so far. But what is the formula for $Var(Y)$?

The outcome of interest is a change or difference between pre- and post-treatment raw outcomes, denoted as $Y = Y_{post} - Y_{pre}$. For any participant, each pre-post outcome may be correlated. For example, if a participant's pre-treatment outcome is high, their post-treatment outcome may increase or decrease, but may also be high. This is called the intraclass correlation (ICC), where a "class" (or cluster) in our case is a participant.

We must account for the ICC in our power and sample size (PSS) calculations. The good news is that this actually can reduce the sample size we'll need!
  
Let $\sigma^2$ represent the variance of raw outcomes at either pre- or post-treatment. Then the SD of the pre-post difference $Y$ is $\sigma_Y = \sqrt{2 \sigma^2 (1 - ICC)}$. From this, we see that the higher the positive ICC, the lower the variance of the pre-post difference. This lowers the sample size we would need if $ICC > 0.5$. Specifically, if $ICC = 0.5$, then the variance we would specify in our PSS calculations is simply $\sigma^2$, the variance of outcomes at either pre or post period.

Let $Y_{x,i,t}$ denote the outcome per participant at either the $t = pre$ or $t = post$ period of observation. Then the per-participant difference is $Y_{x,i} = Y_{x,i,post} - Y_{x,i,pre}$, yielding $Var(Y) = Var \left( Y_{post} - Y_{pre} \right)$. Recall that
\[
  ICC = \frac{Cov \left( Y_{post}, Y_{pre} \right)}{\sqrt{Var \left( Y_{post} \right) Var \left( Y_{pre} \right)}},
\]
as in equation (2) of Eldridge et al (2009) (i.e., the [standard definition of correlation](https://en.wikipedia.org/wiki/Correlation_and_dependence)). Here, we make what Eldridge et al call the "common correlation, compound symmetry, or exchangeability assumption", as defined in Liang and Zeger (1986).

We have:
  \[
    \begin{aligned}
    Var(Y)
    & = Var \left( Y_{post} - Y_{pre} \right) \\
    & = Cov \left( Y_{post} - Y_{pre}, Y_{post} - Y_{pre} \right) \\
    & = Cov \left( Y_{post}, Y_{post} \right) +
      Cov \left( Y_{post}, - Y_{pre} \right) +
      Cov \left( - Y_{pre}, Y_{post} \right) +
      Cov \left( - Y_{pre}, - Y_{pre} \right) \\
    & = Var \left( Y_{post} \right) + Var \left( Y_{pre} \right) - 2 Cov \left( Y_{post}, Y_{pre} \right) \\
    & = Var \left( Y_{post} \right) + Var \left( Y_{pre} \right) - 2 ICC \sqrt{Var \left( Y_{post} \right) Var \left( Y_{pre} \right)}
    \end{aligned}
    \]
If $Var \left( Y_{post} \right) = Var \left( Y_{pre} \right) \equiv \sigma^2$, this simplifies to $Var(Y) = 2 \sigma^2 - 2 ICC \sigma^2 = 2 \sigma^2 (1 - ICC) = \sigma_Y^2$.

<!-- #### Relationship between Cronbach's $\alpha$ and ICC -->
  
  <!-- Following [de Vet et al (2017)](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494), let $\sigma^2_p$ denote the variance in outcomes between participants. Let $\sigma^2_\text{error}$ denote the error variance in de Vet et al; this is just equal to $\sigma^2$ in our notation. -->
  <!-- <!-- Let $\sigma^2_\text{error} = \sigma^2_r + \sigma^2_\text{residual}$ denote the error variance, which is decomposed as the sum of a random error term $\sigma^2_r$ and a systematic error term $\sigma^2_\text{residual}$ for systematic differences among raters. --> -->
  <!-- Let $k$ denote the number of survey questionnaire items; in the MOS Problem Index II, there are $k = 9$ items. -->
  
  <!-- Equation (1) in de Vet et al is -->
  <!-- \[ -->
            <!-- \begin{aligned} -->
            <!--   ICC -->
            <!--     &= \frac{\sigma^2_p}{\sigma^2_p + \sigma^2} -->
            <!-- \end{aligned} -->
            <!-- . -->
            <!-- \] -->
  <!-- Cronbach's $\alpha$ is equation (3): -->
<!-- \[ -->
<!-- \begin{aligned} -->
<!--   \alpha -->
<!--     &= \frac{\sigma^2_p}{\sigma^2_p + \sigma^2_\text{residual}/k} -->
<!-- \end{aligned} -->
<!-- \] -->
<!-- Hence, the ICC equals Cronbach $\alpha$ when $\sigma^2 = \sigma^2_\text{residual}/k$. -->

#### Equivalent Cohen's $d$ Values

Let $d_j = \delta_j \big/ \sqrt{2 \sigma_j^2 (1 - ICC)}$ represent Cohen's $d$ for average difference $d_j$ and outcome SD $\sigma_j$. We have:
\[
\begin{aligned}
d_1
& = d_2 \\
\frac{\delta_1}{\sqrt{2 \sigma_1^2 (1 - ICC)}}
& = \frac{\delta_2}{\sqrt{2 \sigma_2^2 (1 - ICC)}} \\
\frac{\delta_1}{\sigma_1 \sqrt{2 (1 - ICC)}}
& = \frac{\delta_2}{\sigma_2 \sqrt{2 (1 - ICC)}} \\
\frac{\delta_1}{\sigma_1}
& = \frac{\delta_2}{\sigma_2} \\
\delta_1 \frac{\sigma_2}{\sigma_1}
& = \delta_2
\end{aligned}
\]



## Transforming and Analyzing Skewed Variables

Many variables such as lab analytes range over non-negative values, and are skewed in distribution. Suppose we want to conduct a PSS calculation that assumes the relevant variable is normally distributed. For such skewed variables, the original variable must first be transformed from its natural or native scale.

The transformed variable should have moments (i.e., means, standard deviations, etc.) that we can reasonably assume [map to non-parametric summaries](https://en.wikipedia.org/wiki/Log-normal_distribution#Mode,_median,_quantiles) of the original variable (i.e., quantiles, interquartile range [IQR], etc.). This makes it easier to translate natural-scale non-parametric summaries found in the literature to the transformed-variable moments required for PSS calculations.


### Log-Normal Distribution

Suppose $V_t$ follows a log-normal distribution. Then equivalently, $Y_t = \ln( V_t )$ is normally distributed as $N ( \mu_t, \sigma_t )$ at any $t$, with mean $\mu_t$ and standard deviation $\sigma_t$. We have the mean and median of $V_t$, $\mu_{V_t} = \exp \big( \mu_t + \frac{1}{2} \sigma_t^2 \big)$ and $\text{med}_{V_t} = \exp ( \mu_t )$, respectively. The median of $V_t$ is equal to its geometric mean.

For now, suppress the $t$ index. The quantile function of $V$ for quantile or percentile value $p \in (0, 1)$ is $q_V ( p ) = \exp \big( \mu + \sigma \Phi^{-1} (p) \big)$, where $\Phi(z)$ represents the standard normal $N (0, 1)$ cumulative distribution function of $Z$ at $z$. Equivalently:

\begin{align*}
  \mu & = \ln q_V ( p ) - \sigma \Phi^{-1} (p) \\
  \sigma & = \frac{ \ln q_V ( p ) - \mu }{ \Phi^{-1} (p) }
\end{align*}

The standard deviation $\sigma$ in terms of a given quantile $p$, $V$-quantile value $q_V ( p )$, and $\text{med}_V$ is:

\begin{align*}
  q_V ( p ) & = \exp \left( \mu + \sigma \Phi^{-1} (p) \right) \\
  q_V ( p ) & = \exp ( \mu ) \exp \left( \sigma \Phi^{-1} (p) \right) \\
  q_V ( p ) & = \text{med}_V \exp \left( \sigma \Phi^{-1} (p) \right) \\
  \frac{ q_V ( p ) }{\text{med}_V} & = \exp \left( \sigma \Phi^{-1} (p) \right) \\
  \ln \left( \frac{ q_V ( p ) }{\text{med}_V} \right) & = \sigma \Phi^{-1} (p) \\
  \ln \left( \frac{ q_V ( p ) }{\text{med}_V} \right) \frac{1}{\Phi^{-1} (p)} & = \sigma
\end{align*}

Let us now re-introduce the $t$ index. Suppose the pre-post mean difference in original values, denoted $V = V_{post} - V_{pre}$, follows a log-normal distribution. Then equivalently, $Y = \ln( V )$ is normally distributed as $N ( \mu_Y, \sigma_Y )$, with mean $\mu_Y$ and standard deviation $\sigma_Y$. We have the mean and median of $V$, $\mu_V = \exp \big( \mu_Y + \frac{1}{2} \sigma_Y^2 \big)$ and $\text{med}_V = \exp ( \mu_Y )$, respectively. The median of $V$ is equal to its geometric mean.

Equivalently:

\begin{align*}
  q_V ( p ) & = \exp \left( \mu_Y + \sigma_Y \Phi^{-1} (p) \right) \\
  \mu_Y & = \ln q_V ( p ) - \sigma_Y \Phi^{-1} (p) \\
  \sigma_Y & = \frac{ \ln q_V ( p ) - \mu_Y }{ \Phi^{-1} (p) } \\
  & = \ln \left( \frac{ q_V ( p ) }{\text{med}_V} \right) \frac{1}{\Phi^{-1} (p)}
\end{align*}

Let $\text{diff}_{\text{med}} = \text{med}_{V_{post}} - \text{med}_{V_{pre}}$ denote the difference between the median original variable $V$ at $t = post$ versus $t = pre$. Suppose this difference in medians $\text{diff}_{\text{med}}$ is equal to the median pre-post difference $\text{med}_V$; i.e., suppose $\text{diff}_{\text{med}} = \text{med}_V$. Then the corresponding mean of the transformed pre-post differences is $\mu_Y = \ln \text{diff}_{\text{med}}$. Recall that for one group of participants, $\mu_Y = \delta$.



## Why and How to Drop the Phrase "Statistical Significance"

From above, notice that statistical significance strictly applies to p-values, not estimates. So phrases like "there was a statistically significant difference" are ill-defined. Such phrases are also scientifically disingenuous because they are often interpreted as "there was a significant or scientifically important difference". However, there is nothing in the definition of statistical significance about significance (unqualified) or scientific importance.

Put simply, statistical significance has nothing to do with significance. The phrase "statistical significance" unfortunately has come to perpetuate institutionalized intellectual dishonesty. It has led to the current global replication crisis in biomedicine, psychology, and beyond---a result of decades of publishing and "file-drawer" bias due to misaligned incentives that conflate "statistical significance" with "scientific importance" while paying lip service to doing no such thing.

Some alternatives are to shorten "statistically significant" to simply "statistical"---or better yet, to "discernible", "detectable", "apparent", "evident", or "inferrable". For solutions, please see [twitter.com/ericjdaza/status/1382413617572708356](https://twitter.com/ericjdaza/status/1382413617572708356). I have successfully published a paper within minimal use of the phrase "statistical significance"; see [twitter.com/ericjdaza/status/1372851369317568512](https://twitter.com/ericjdaza/status/1372851369317568512) for how I did it.

In general, I highly recommend citing [Witmer (2019)](https://www.tandfonline.com/doi/full/10.1080/10691898.2019.1702415), [McShane et al (2019)](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253), [Wasserstein et al (2019)](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913), and [Wasserstein and Lazar (2016)](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108) (see **References**) as the definitive/authoritative leading statistical science references in support of this lexical strategy. They can also be used to carefully but firmly rebuff reviewer suggestions to keep "statistical significance".



## Appendix References

- Eldridge SM, Ukoumunne OC, Carlin JB. The intra‐cluster correlation coefficient in cluster randomized trials: a review of definitions. International Statistical Review. 2009 Dec;77(3):378-94. [onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2009.00092.x](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2009.00092.x)

- Liang KY, Zeger SL. Longitudinal data analysis using generalized linear models. Biometrika. 1986 Apr 1;73(1):13-22. [academic.oup.com/biomet/article/73/1/13/246001](https://academic.oup.com/biomet/article/73/1/13/246001)

- McShane BB, Gal D, Gelman A, Robert C, Tackett JL. Abandon statistical significance. The American Statistician. 2019 Mar 29;73(sup1):235-45. [amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253)

<!-- - de Vet HC, Mokkink LB, Mosmuller DG, Terwee CB. Spearman–Brown prophecy formula and Cronbach's alpha: different faces of reliability and opportunities for new applications. Journal of Clinical Epidemiology. 2017 May 1;85:45-9. [sciencedirect.com/science/article/abs/pii/S0895435617302494](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494) -->
  
- Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. "Moving to a world beyond “p< 0.05”." (2019): 1-19. [tandfonline.com/doi/full/10.1080/00031305.2019.1583913](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913)

- Wasserstein, Ronald L., and Nicole A. Lazar. "The ASA statement on p-values: context, process, and purpose." (2016): 129-133. [tandfonline.com/doi/full/10.1080/00031305.2016.1154108](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)

- Witmer, J. (2019). Editorial. Journal of Statistics Education, 27(3), 136–137. [doi.org/10.1080/10691898.2019.1702415](https://www.tandfonline.com/doi/full/10.1080/10691898.2019.1702415)



## ((INTERNAL USE ONLY)) Learnings: Process Lessons Learned ((optional but recommended))

Add lessons learned here. Examples include things that add time to doing these calculations, like:

- parameters (e.g., means, standard deviations) having to be derived from the literature because they aren't reported explicitly, with explanations and justifications for why your derivations are reasonable

- different pre-post time intervals between what's in the literature and your study design; may require adjustment (e.g., linear extrapolation) if the anticipated effect or association size depends on the time between pre- and post-

- more complex designs, like a cluster randomized trial (which necessitates setting an ICC between participants within each cluster)

- having to transform a skewed variable before it can be used for PSS calculations