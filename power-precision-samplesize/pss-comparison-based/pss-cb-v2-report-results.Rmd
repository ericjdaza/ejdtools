---
title: "Project: The VICS21 Study: Power and Sample Size Calculations"
author: "[Eric J. Daza, DrPH, MPS](https://www.ericjdaza.com/)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
---


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, results='hide', message=FALSE}
# Project: [insert project name]
#   Links: [insert project folder link]
# 
#   Description: Power and sample size justification for common requests with vague research
#     hypotheses involving comparisons (e.g., one-sample, two-sample) and possible multiple
#     testing (i.e., multiplicity).




# Preliminary processing


## Clear environment and install/load packages.
rm(list=ls())
if (!require("pacman")) install.packages("pacman") # just to make sure "pacman" is installed
pacman::p_load(tidyverse, knitr, ggpubr, tinytex) # e.g., naniar, extrafont, tidyverse, reshape2, janitor, lubridate, jsonlite, arsenal, knitr, feather


## Set global parameters.
scalar_seed <- # optional: specify random number seed for reproducibility
output_path <- "/LocalPath/data/MyProjectFolder/" # your local path and project folder within the "data" folder


## Load datasets or data image.

tbl_pss_parameters <- readr::read_csv(paste0(output_path, "tbl_pss_parameters.csv"))

tbl_sims_final_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_final_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(type_of_test == "two.sample")
tbl_sims_fancy_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_fancy_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(`Type of Test` == "two-sample") %>% dplyr::select(-`Type of Test`)
tbl_sims_sentences_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_sentences_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(type_of_test == "two.sample") %>% dplyr::select(-type_of_test)

# tbl_sims_final_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_final_ntests", tbl_pss_parameters$num_of_tests, ".csv"))
# tbl_sims_fancy_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_fancy_ntests", tbl_pss_parameters$num_of_tests, ".csv"))
# tbl_sims_sentences_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_sentences_ntests", tbl_pss_parameters$num_of_tests, ".csv"))


## Choose scenario.
chosen_alpha_familywise <- 0.10
chosen_power_level <- 0.80
chosen_icc <- tbl_pss_parameters$chosen_icc
tbl_sims_final_chosen <- tbl_sims_final_ntests %>%
  dplyr::filter(
    alpha_familywise == chosen_alpha_familywise,
    power_level == chosen_power_level,
    icc == chosen_icc
  ) %>%
  dplyr::select(Scenario, sampsize, sampsize_attrition)
tbl_sims_sentence_chosen <- tbl_sims_sentences_ntests %>%
  dplyr::filter(Scenario == tbl_sims_final_chosen$Scenario)
```





# Objective

We want to statistically discern or detect a hypothesized true average difference (i.e., association or effect size) of size $\delta$ or larger in continuous outcomes $Y$ between `r tbl_pss_parameters$num_of_tests` treatment groups. We will assume these outcomes vary with the same standard deviation (SD) $\sigma_Y$ in both groups. These two quantities are often combined as Cohen's $d$, defined as $d = \delta \big/ \sigma_Y$. We will also assume these outcomes are fairly normally distributed, allowing us to rely on Cohen's $d$ and the two-sample t-test in our calculations carried out via the R command `pwr::pwr.t.test()`.





# Methods



## Basic Setup and Parameters

To do so requires a certain sample size $n$, which depends on the following two quantities.

1. What is the highest chance of accidentally discerning/detecting a false average difference of size $\delta$ or larger, that we will allow or tolerate? This is the maximum false-positive or "Type 1 error" rate we want, formally written as $\alpha$.
    - The false-positive rate (FPR) is defined as the probability of discerning/detecting a false average difference of this size by chance.

1. For a given $\alpha$, what is the lowest chance of discerning/detecting a true average difference of size $\delta$ or larger, that we will allow or tolerate? This is the statistical power we want, formally written as $1 - \beta$.
    - Statistical power is defined as the probability of discerning/detecting a true average difference of this size for a given FPR. Put differently, it is the power to statistically discern/detect a true average difference of this size for a given FPR.

Note that $\alpha$ also equals our desired threshold for declaring a test's p-value to be statistically significant. That is, we decide beforehand that if a test's p-value is no larger than $\alpha$, then we can be confident that our sample average difference is an estimate of a true, unknown average difference that is not zero. How confident? $(1 - \alpha) \times 100\%$, which for $\alpha = 0.05$ equals 95%. We can now see how this $\alpha$ exactly corresponds to the degree of confidence expressed in a confidence interval.

In formal deliverables and manuscripts, we will avoid using the phrase "statistical significance" except when first defining the alternate, correct term to use. Instead, we will use the phrase "statistically discernible" or "statistically supported". (See **Appendix: Why and How to Drop the Phrase "Statistical Significance"** for motivation, details, and resources/references we will cite.) We will use the phrase "statistical significance" only if absolutely required by the client or reviewing body/organization.

Furthermore, if our estimation method or "estimator" is statistically consistent (which standard estimators are), then we can further conclude that a given estimate is unbiased for the true average difference with a large enough sample. This hypothesized true average difference is exactly what we use to determine the statistical power we want.



## Adjustment Parameters


### Number of Tests

There is often more than one pre-planned statistical hypothesis test. The number of such tests corresponds with the number of research hypothesis findings that we would like to claim generalize beyond our study; i.e., generalizable findings are reproducible, meaning they can be approximately reproduced by another similar study with a different sample. These are called *a priori* or *ex ante* hypotheses because we document them before collecting or analyzing any study data. Analysis findings from a priori hypotheses are the most generalizable, compared to hypotheses generated or analysis adjusted after starting to collect or analyze data. See **Appendix: Confirmatory vs. Exploratory Objectives** for why.

The desired generalizable findings often correspond to a study's main research hypotheses or endpoints. If pre-study (i.e., a priori, ex ante) research interests include a more elaborate linearized model, sometimes researchers want to claim that the estimated coefficients of certain model terms generalize. The number of terms in such models must be included in the total number of tests to run.

For $m$ tests, we must adjust the FPR for each test in a way that preserves the overall or familywise $\alpha$ (as in the "family" of $m$ tests). This is called *multiplicity adjustment*. The easiest such adjustment is a simple *Bonferroni correction*, which equally splits the familywise $\alpha$ in $m$; i.e., each test's FPR is equal to $\alpha / m$.


### Repeated Measures: Intraclass Correlation

The outcome of interest is a change or difference between pre- and post-treatment raw outcomes, denoted as $Y = Y_{post} - Y_{pre}$. For any participant, each pre-post outcome may be correlated. For example, if a participant's pre-treatment outcome is high, their post-treatment outcome may increase or decrease, but may also be high. This is called the intraclass correlation (ICC), where a "class" (or cluster) in our case is a participant.

We must account for the ICC in our power and sample size (PSS) calculations. The good news is that this actually can reduce the sample size we'll need!

Let $\sigma^2$ represent the variance of raw outcomes in either group at either pre- or post-treatment. Then the SD of the pre-post difference $Y$ in either group is $\sigma_Y = \sqrt{2 \sigma^2 (1 - ICC)}$ (see **Appendix: Repeated Measures**). From this, we see that the higher the positive ICC, the lower the variance of the pre-post difference. This lowers the sample size we'd need if $ICC > 0.5$. Specifically, if $ICC = 0.5$, then the variance we'd specify in our PSS calculations is simply $\sigma^2$, the variance of outcomes at either pre or post period.

### Anticipated Attrition

Some participants may drop out of the study before the end of the study period. We say they "dropped out" or the study, or are "lost to follow-up", a process also called study sample attrition.

We will need to enroll a larger sample than needed to account for the maximum amount of anticipated attrition. This may occur, for example, due to non-engagement, non-adherence, or non-compliance with the treatment.

Let $p_{att}$ represent the anticipated proportion of attrition. The minimum larger sample needed to accommodate $p_{att} \times 100$% attrition is equal to $n_{adj} = n / (1 - p_{att})$.





# Results: Sample Sizes Needed



## Parameters Chosen

The most conservative sample size will be the largest one we anticipate needing. To calculate this, we need to use the smallest hypothesized true average difference between the `r tbl_pss_parameters$num_of_tests` research hypotheses, along with the outcome with the largest SD. Based on our literature review and scientific judgement, we will assume that the smallest true average difference is `r tbl_pss_parameters$delta_primary` units, and that the largest SD of pre- and post-treament outcomes in both groups is `r tbl_pss_parameters$sd_primary`. The same sample size results will apply if the smallest true average difference is `r tbl_pss_parameters$delta_secondary %>% round(2)` units, and the largest SD of pre- and post-treament outcomes in both groups is `r tbl_pss_parameters$sd_secondary`.

We will conduct `r tbl_pss_parameters$num_of_tests` main statistical hypothesis tests. These correspond to our primary and secondary research hypotheses or endpoints. We will use a Bonferroni correction that equally splits the familywise $\alpha$ in `r tbl_pss_parameters$num_of_tests`; i.e., each test's $\alpha$ is equal to $\alpha$/`r tbl_pss_parameters$num_of_tests`.

<!-- The literature supports an ICC of 0.78 for the Medical Outcomes Study (MOS) Sleep Scale Problems Index II 9-item subscale. This is based on using Cronbach's $\alpha$ as a proxy for the ICC ([de Vet et al, 2017](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494)). The relevant Cronbach's $\alpha$ value is 0.78 in the last row of Table 2 of [Allen et al (2009)](https://www.sciencedirect.com/science/article/abs/pii/S1389945708001627) as a reasonable proxy. We will assume that at most `r tbl_pss_parameters$p_attrition * 100`% of participants on each arm may drop out before study completion. -->

The literature supports an ICC of `r chosen_icc` for the Medical Outcomes Study (MOS) Sleep Scale Problems Index II 9-item subscale. This is based on using the smallest MOS test-retest ICC of [Rejas et al (2007)](https://www.sciencedirect.com/science/article/abs/pii/S1090380106000681). We will assume that at most `r tbl_pss_parameters$p_attrition * 100`% of participants on each arm may drop out before study completion.



## Main Recommendation

*We will enroll at least `r tbl_sims_final_chosen$sampsize_attrition` participants per arm. `r tbl_sims_sentence_chosen$Description`*



## Other Scenarios

The naive Cohen's $d$ (i.e., before adjusting for ICC, or equivalently setting the ICC to 0.5) is `r tbl_pss_parameters$cohens_d_naive %>% round(3)`. The Cohen's $d$ we will use accounts for the ICC, and is equal to $d = \delta \big/ \sqrt{2 \sigma^2 (1 - ICC)}$.

Here is an example of two extremes of required sample sizes for two-sample tests:

- `r tbl_sims_sentences_ntests$Description[tbl_sims_final_ntests$sampsize == min(tbl_sims_final_ntests$sampsize, na.rm = TRUE)]`
- `r tbl_sims_sentences_ntests$Description[tbl_sims_final_ntests$sampsize == max(tbl_sims_final_ntests$sampsize, na.rm = TRUE)]`

The trend between these extremes is illustrated below.
- For any particular tolerable overall FPR, the number of participants required in each group increases with higher desired statistical power.
- For any particular FPR and power, the number of participants decreases with higher ICC.

```{r echo=FALSE, out.width="100%", fig.cap="One Sample: One Statistical Test"}
tbl_sims_fancy_ntests %>% knitr::kable("simple")
```





# Appendix



## Why and How to Drop the Phrase "Statistical Significance"

From above, notice that statistical significance strictly applies to p-values, not estimates. So phrases like "there was a statistically significant difference" are ill-defined. Such phrases are also scientifically disingenuous because they are often interpreted as "there was a significant or scientifically important difference". However, there is nothing in the definition of statistical significance about significance (unqualified) or scientific importance.

Put simply, statistical significance has nothing to do with significance. The phrase "statistical significance" unfortunately has come to perpetuate institutionalized intellectual dishonesty. It has led to the current global replication crisis in biomedicine, psychology, and beyond---a result of decades of publishing and "file-drawer" bias due to misaligned incentives that conflate "statistical significance" with "scientific importance" while paying lip service to doing no such thing.

Some alternatives are to shorten "statistically significant" to simply "statistical"---or better yet, to "discernible", "detectable", "apparent", "evident", or "inferrable". For solutions, please see [twitter.com/ericjdaza/status/1382413617572708356](https://twitter.com/ericjdaza/status/1382413617572708356). I have successfully published a paper within minimal use of the phrase "statistical significance"; see [twitter.com/ericjdaza/status/1372851369317568512](https://twitter.com/ericjdaza/status/1372851369317568512) for how I did it.

In general, I highly recommend citing [McShane et al (2019)](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253), [Wasserstein et al (2019)](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913), and [Wasserstein and Lazar (2016)](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108) (see **References**) as the definitive/authoritative leading statistical science references in support of this lexical strategy. They can also be used to carefully but firmly rebuff reviewer suggestions to keep "statistical significance".



## Confirmatory vs. Exploratory Objectives

Confirmatory study objectives seek to test or confirm a priori hypotheses. They are written with the intent to make the claim that this particular study's findings further confirm those a priori hypotheses. In theory, if these findings "confirm" the a priori hypotheses, this result strengthens those hypotheses by providing more evidence that they hold true more generally. Hence, the findings should be generalizable.

After we collect or analyze the study data, we may come up with (i.e., generate) more realistic *post hoc* or *ex post* scientific hypotheses. This is called "exploratory analysis, and often happens by adjusting tests or models after the first time the test is conducted or model is fit. But these post hoc hypotheses carry the risk of overfitting; i.e., these seemingly more realstic hypotheses that seem to "fit the data better" may not be reproducible because they are idiosyncratic to our particular sample. The only way to properly assess the degree of idiosyncracy is by collecting another sample and fitting the post hoc model, doing this every time we adjust the model.

This overfitting process is also called "hypothesizing after the results are known" (HARKing). An very infamous version of HARKing is called *p-hacking*, whereby this post hoc hypothesizing is specifically carried out to find and report findings that are "statistically significant"---but may be clinically or scientifically useless or meaningless. (See **Appendix: Why and How to Drop the Phrase "Statistical Significance"**.)

A good way to safeguard these is to describe in as much detail as possible the possible types of exploratory analyses we might run. We would list these in our study protocol under a section called something like "exploratory objectives" or "exploratory endpoints".



## Repeated Measures


### T-Statistic: Outcome Variance

Let $X_i = 1$ if participant $i$ is assigned to active treatment, and let $X_i = 0$ if they're assigned to placebo control. For either treatment $x \in {0, 1}$, let $Y_{x,i}$ denote the pre-post difference in outcomes for participant $i$. Let $\mu_x = E(Y_i | X_i=x)$ denote the mean pre-post difference for treatment $x$, which we assume is the same for all participants receiving $x$.

We want to estimate $\delta = \mu_1 - \mu_0$. The empirical average of the mean outcome for each treatment is $\hat\mu_x = \frac{1}{n_x} \sum_{i=1}^{n_x} Y_{x,i}$. Let $\hat\delta = \hat\mu_1 - \hat\mu_0$ denote our $\delta$ estimator.

The t-statistic we need to do our PSS calculations requires specifying both $\delta$ and $Var \left( \hat\delta \right)$. We assume the pre-post difference has the same variance no matter the treatment; i.e., we assume $Var \left( Y_{1,i} \right) = Var \left( Y_{0,i} \right) = Var(Y)$ for any participant $i$.

Note that we actually specify two $\delta$'s for PSS calculations; the null hypothesis $\delta_{null} = 0$ and the alternative hypothesis $\delta_{alt} \ne 0$, which we set to our effect size of interest. We will assume $Var \left( \hat\delta_{null} \right) = Var \left( \hat\delta_{alt} \right) = Var \left( \hat\delta \right)$.

We have:
\[
\begin{aligned}
Var \left( \hat\delta \right)
& = Var \left( \hat\mu_1 - \hat\mu_0 \right) \\
& = Var \left(
  \frac{1}{n_1} \sum_{i=1}^{n_1} Y_{1,i} -
  \frac{1}{n_0} \sum_{i=1}^{n_0} Y_{0,i}
  \right) \\
& = \frac{1}{n_1} Var \left( Y_{1,i} \right) + \frac{1}{n_0} Var \left( Y_{0,i} \right) \\
& = \left( \frac{1}{n_1} + \frac{1}{n_0} \right) Var(Y)
\end{aligned}
\]


### Intraclass Correlation

The t-statistic variance formula has been straightforward so far. But what is the formula for $Var(Y)$?

Let $Y_{x,i,pre}$ and $Y_{x,i,post}$ denote the two sequential measures per participant during the pre and post periods, respectively. Then the per-participant difference is $Y_{x,i} = Y_{x,i,post} - Y_{x,i,pre}$, yielding $Var(Y) = Var \left( Y_{post} - Y_{pre} \right)$. Recall that
\[
ICC = \frac{Cov \left( Y_{post}, Y_{pre} \right)}{\sqrt{Var \left( Y_{post} \right) Var \left( Y_{pre} \right)}}
.
\]

We have:
\[
\begin{aligned}
Var(Y)
& = Var \left( Y_{post} - Y_{pre} \right) \\
& = Cov \left( Y_{post} - Y_{pre}, Y_{post} - Y_{pre} \right) \\
& = Cov \left( Y_{post}, Y_{post} \right) +
  Cov \left( Y_{post}, - Y_{pre} \right) +
  Cov \left( - Y_{pre}, Y_{post} \right) +
  Cov \left( - Y_{pre}, - Y_{pre} \right) \\
& = Var \left( Y_{post} \right) + Var \left( Y_{pre} \right) - 2 Cov \left( Y_{post}, Y_{pre} \right) \\
& = Var \left( Y_{post} \right) + Var \left( Y_{pre} \right) - 2 ICC \sqrt{Var \left( Y_{post} \right) Var \left( Y_{pre} \right)}
\end{aligned}
\]
If $Var \left( Y_{post} \right) = Var \left( Y_{pre} \right) \equiv \sigma^2$, this simplifies to $Var(Y) = 2 \sigma^2 - 2 ICC \sigma^2 = 2 \sigma^2 (1 - ICC)$.

<!-- #### Relationship between Cronbach's $\alpha$ and ICC -->

<!-- Following [de Vet et al (2017)](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494), let $\sigma^2_p$ denote the variance in outcomes between participants. Let $\sigma^2_\text{error}$ denote the error variance in de Vet et al; this is just equal to $\sigma^2$ in our notation. -->
<!-- <!-- Let $\sigma^2_\text{error} = \sigma^2_r + \sigma^2_\text{residual}$ denote the error variance, which is decomposed as the sum of a random error term $\sigma^2_r$ and a systematic error term $\sigma^2_\text{residual}$ for systematic differences among raters. --> -->
<!-- Let $k$ denote the number of survey questionnaire items; in the MOS Problem Index II, there are $k = 9$ items. -->

<!-- Equation (1) in de Vet et al is -->
<!-- \[ -->
<!-- \begin{aligned} -->
<!--   ICC -->
<!--     &= \frac{\sigma^2_p}{\sigma^2_p + \sigma^2} -->
<!-- \end{aligned} -->
<!-- . -->
<!-- \] -->
<!-- Cronbach's $\alpha$ is equation (3): -->
<!-- \[ -->
<!-- \begin{aligned} -->
<!--   \alpha -->
<!--     &= \frac{\sigma^2_p}{\sigma^2_p + \sigma^2_\text{residual}/k} -->
<!-- \end{aligned} -->
<!-- \] -->
<!-- Hence, the ICC equals Cronbach $\alpha$ when $\sigma^2 = \sigma^2_\text{residual}/k$. -->

#### Equivalent Cohen's $d$ Values

Let $d_j = \delta_j \big/ \sqrt{2 \sigma_j^2 (1 - ICC)}$ represent Cohen's $d$ for average difference $d_j$ and outcome SD $\sigma_j$. We have:
\[
\begin{aligned}
d_1
& = d_2 \\
\frac{\delta_1}{\sqrt{2 \sigma_1^2 (1 - ICC)}}
& = \frac{\delta_2}{\sqrt{2 \sigma_2^2 (1 - ICC)}} \\
\frac{\delta_1}{\sigma_1 \sqrt{2 (1 - ICC)}}
& = \frac{\delta_2}{\sigma_2 \sqrt{2 (1 - ICC)}} \\
\frac{\delta_1}{\sigma_1}
& = \frac{\delta_2}{\sigma_2} \\
\delta_1 \frac{\sigma_2}{\sigma_1}
& = \delta_2
\end{aligned}
\]
If $\delta_1$ = `r tbl_pss_parameters$delta_primary`, $\sigma_1$ = `r tbl_pss_parameters$sd_primary`, and $\sigma_2$ = `r tbl_pss_parameters$sd_secondary`, then $\delta_2$ = `r tbl_pss_parameters$delta_secondary %>% round(2)`.





# Process Lessons Learned (Learnings)

Add lessons learned here. Examples include things that add time, like:
- parameters (e.g., means, standard deviations) having to be derived from the literature because they aren't reported explicitly, with explanations and justifications for why your derivations are reasonable
- more complex designs, like a cluster randomized trial (which necessitates setting an ICC between participants within each cluster)






# References

<!-- - Allen RP, Kosinski M, Hill-Zabala CE, Calloway MO. Psychometric evaluation and tests of validity of the Medical Outcomes Study 12-item Sleep Scale (MOS sleep). Sleep medicine. 2009 May 1;10(5):531-9. [sciencedirect.com/science/article/abs/pii/S1389945708001627](https://www.sciencedirect.com/science/article/abs/pii/S1389945708001627) -->

- Ashbaugh AR, Houle-Johnson S, Herbert C, El-Hage W, Brunet A. Psychometric validation of the English and French versions of the Posttraumatic Stress Disorder Checklist for DSM-5 (PCL-5). PloS one. 2016 Oct 10;11(10):e0161645. [journals.plos.org/plosone/article?id=10.1371/journal.pone.0161645](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0161645)

- Blevins CA, Weathers FW, Davis MT, Witte TK, Domino JL. The posttraumatic stress disorder checklist for DSM‐5 (PCL‐5): Development and initial psychometric evaluation. Journal of traumatic stress. 2015 Dec;28(6):489-98. [dx.doi.org/10.1002/jts.22059](http://dx.doi.org/10.1002/jts.22059)

- Bovin MJ, Marx BP, Weathers FW, Gallagher MW, Rodriguez P, Schnurr PP, Keane TM. Psychometric properties of the PTSD checklist for diagnostic and statistical manual of mental disorders–fifth edition (PCL-5) in veterans. Psychological assessment. 2016 Nov;28(11):1379. [ptsd.va.gov/professional/articles/article-pdf/id44666.pdf](http://www.ptsd.va.gov/professional/articles/article-pdf/id44666.pdf)

- McShane BB, Gal D, Gelman A, Robert C, Tackett JL. Abandon statistical significance. The American Statistician. 2019 Mar 29;73(sup1):235-45. [amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2018.1527253)

- Rejas J, Ribera MV, Ruiz M, Masrramón X. Psychometric properties of the MOS (Medical Outcomes Study) Sleep Scale in patients with neuropathic pain. European Journal of Pain. 2007 Apr 1;11(3):329-40. [sciencedirect.com/science/article/abs/pii/S1090380106000681](https://www.sciencedirect.com/science/article/abs/pii/S1090380106000681)

<!-- - de Vet HC, Mokkink LB, Mosmuller DG, Terwee CB. Spearman–Brown prophecy formula and Cronbach's alpha: different faces of reliability and opportunities for new applications. Journal of Clinical Epidemiology. 2017 May 1;85:45-9. [sciencedirect.com/science/article/abs/pii/S0895435617302494](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494) -->

- Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. "Moving to a world beyond “p< 0.05”." (2019): 1-19. [tandfonline.com/doi/full/10.1080/00031305.2019.1583913](https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1583913)

- Wasserstein, Ronald L., and Nicole A. Lazar. "The ASA statement on p-values: context, process, and purpose." (2016): 129-133. [tandfonline.com/doi/full/10.1080/00031305.2016.1154108](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)