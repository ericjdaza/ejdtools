---
title: "Project: [insert project name or codename]: Power and Sample Size Calculations for One Group (Paired Samples)"
author: "[Eric J. Daza, DrPH, MPS | ericjdaza.com](https://www.ericjdaza.com/)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
---


```{r include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, results='hide', message=FALSE}
# Calculation Characteristics:
#   mean known
#   SD known
#   ICC known or unknown
# 
# Project: [insert project name]
#   Links: [insert project folder link]
# 
#   Description: Power and sample size justification for common requests with vague research
#     hypotheses involving comparisons (e.g., one-sample, two-sample) and possible multiple
#     testing (i.e., multiplicity).




# Preliminary processing


## Clear environment and install/load packages.
rm(list=ls())
if (!require("pacman")) install.packages("pacman") # just to make sure "pacman" is installed
pacman::p_load(tidyverse, knitr, ggpubr, tinytex) # e.g., naniar, extrafont, tidyverse, reshape2, janitor, lubridate, jsonlite, arsenal, knitr, feather


## Load custom tools.
utils <- "/Users/ericjaydaza/Documents/Github/[project_codename]/code/0-utils/"
files_sources <- list.files(utils)
files_sources <- files_sources[grepl(".r", tolower(files_sources))]
sapply(paste0(utils, files_sources), source) %>% invisible()
rm(utils, files_sources)


## Read config file. The part of the path before "output/config.json"
## should exactly match config$path_data.
config <- jsonlite::fromJSON("/Users/ericjaydaza/Documents/Github/[project_codename]/data/output/config.json")
str(config)


## Set global parameters.
scalar_seed <- # optional: specify random number seed for reproducibility
output_path <- config$path_data # your local path and project folder within the "data" folder


## Load datasets or data image.

tbl_pss_parameters <- readr::read_csv(paste0(output_path, "tbl_pss_parameters.csv"))

tbl_sims_final_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_final_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(type_of_test == "paired")
tbl_sims_fancy_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_fancy_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(`Type of Test` == "paired-samples") %>% dplyr::select(-`Type of Test`)
tbl_sims_sentences_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_sentences_ntests", tbl_pss_parameters$num_of_tests, ".csv")) %>% dplyr::filter(type_of_test == "paired") %>% dplyr::select(-type_of_test)

# tbl_sims_final_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_final_ntests", tbl_pss_parameters$num_of_tests, ".csv"))
# tbl_sims_fancy_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_fancy_ntests", tbl_pss_parameters$num_of_tests, ".csv"))
# tbl_sims_sentences_ntests <- readr::read_csv(paste0(output_path, "tbl_sims_sentences_ntests", tbl_pss_parameters$num_of_tests, ".csv"))


## Choose scenario.
chosen_alpha_familywise <- 0.10
chosen_power_level <- 0.80
chosen_icc <- tbl_pss_parameters$chosen_icc
tbl_sims_final_chosen <- tbl_sims_final_ntests %>%
  dplyr::filter(
    alpha_familywise == chosen_alpha_familywise,
    power_level == chosen_power_level,
    icc == chosen_icc
  ) %>%
  dplyr::select(Scenario, sampsize, sampsize_attrition)
tbl_sims_sentence_chosen <- tbl_sims_sentences_ntests %>%
  dplyr::filter(Scenario == tbl_sims_final_chosen$Scenario)
```





# Executive Summary ((optional))





# Background

Refer to the study protocol.





# Objective

We want to statistically discern or detect a hypothesized true average non-zero pre-post difference (i.e., change over time, effect size) of size $\delta$ or larger in continuous outcomes $Y$. We will assume these outcomes vary with the same standard deviation (SD). We will also assume these outcomes are fairly normally distributed, allowing us to rely on the one-sample t-test in our calculations carried out via the R command `stats::power.t.test()`.
These two quantities are often combined as Cohen's $d$, defined as $d = \delta \big/ \sigma_Y$ where $\sigma_Y$ represents the standard deviation of the pre-post differences.





# Results



## Parameters Chosen

<!-- The most conservative sample size will be the largest one we anticipate needing. To calculate this, we need to use the smallest hypothesized true average difference for the a priori research hypothesis, along with the largest hypothesized true outcome SD. Based on our literature review, pilot study data, and scientific judgment, we will assume that the smallest true average difference is $\delta =$ `r tbl_pss_parameters$delta_primary` units, and that the largest SD of pre- and post-treatment outcomes is `r tbl_pss_parameters$sd_primary`. (Note that the latter is not the same as the SD of the per-participant pre-post difference in outcomes, $\sigma_Y$. See the **Appendix** for details.) -->

The most conservative sample size will be the largest one we anticipate needing. To calculate this, we need to use the smallest hypothesized true average difference among the `r tbl_pss_parameters$num_of_tests` a priori research hypotheses, along with the largest hypothesized true outcome SD. Based on our literature review and scientific judgment, we will assume that the smallest true average difference is $\delta =$ `r tbl_pss_parameters$delta_primary` units, and that the largest SD of pre- and post-treatment outcomes in both groups is `r tbl_pss_parameters$sd_primary`. (Note that the latter is not the same as the SD of the per-participant pre-post difference in outcomes, $\sigma_Y$. See the **Appendix** for details.) The same sample size results will apply if the smallest true average difference is `r tbl_pss_parameters$delta_secondary %>% round(2)` units, and the largest SD of pre- and post-treatment outcomes in both groups is `r tbl_pss_parameters$sd_secondary`.

We will conduct `r tbl_pss_parameters$num_of_tests` main statistical hypothesis tests. These correspond to our primary and secondary research hypotheses or endpoints. We will use a Bonferroni correction that equally splits our chosen maximum familywise false positive rate (FPR) (equivalently, statistical significance level) of $\alpha = $ `r chosen_alpha_familywise` among `r tbl_pss_parameters$num_of_tests` tests; i.e., each test's $\alpha$ is equal to $\alpha$/`r tbl_pss_parameters$num_of_tests`.
<!-- We will conduct one main statistical hypothesis test. These correspond to our primary and secondary research hypotheses or endpoints. We will use a maximum false positive rate (FPR) (equivalently, statistical significance level) of $\alpha =$ `r chosen_alpha_familywise`. -->

<!-- ((example ICC text 1)) The literature supports a pre-post intraclass correlation (ICC) per participant of `r chosen_icc` for the StudySurvey scale. This is based on using Cronbach's $\alpha$ as a proxy for the ICC ([de Vet et al, 2017](https://www.sciencedirect.com/science/article/abs/pii/S0895435617302494)). The relevant Cronbach's $\alpha$ value is ??? in the ??? row of Table ??? of [study domain reference](link) as a reasonable proxy. -->

((example ICC text 2)) The literature supports a pre-post intraclass correlation (ICC) per participant of `r chosen_icc` for the StudySurvey scale. This is based on using the smallest StudySurvey test-retest ICC of [study domain reference](link).

We will assume that at most `r tbl_pss_parameters$p_attrition * 100`% of participants may drop out before study completion.



## Main Recommendation

*We will enroll at least `r tbl_sims_final_chosen$sampsize_attrition`. `r tbl_sims_sentence_chosen$Description`*



## Other Scenarios

The naive Cohen's $d$ (i.e., before adjusting for ICC, or equivalently setting the ICC to 0.5) is `r tbl_pss_parameters$cohens_d_initial %>% round(3)`. The Cohen's $d$ we will use accounts for the ICC, and is equal to $d = \delta \big/ \sqrt{2 \sigma^2 (1 - ICC)}$, where $\sigma^2$ represents the variance of raw outcomes at either pre- or post-treatment (see Appendix).

Here are two examples of the minimum and maximum required sample sizes among different scenarios shown in the table below. Each scenario represents different parameter values.

- `r tbl_sims_sentences_ntests$Description[tbl_sims_final_ntests$sampsize == min(tbl_sims_final_ntests$sampsize, na.rm = TRUE)]`
- `r tbl_sims_sentences_ntests$Description[tbl_sims_final_ntests$sampsize == max(tbl_sims_final_ntests$sampsize, na.rm = TRUE)]`

Note that:

- For a given FPR, the required sample size increases with higher required power.
- For a given FPR and power, the required sample size decreases with higher ICC.

```{r echo=FALSE, out.width="100%", fig.cap="One Sample: One Statistical Test"}
tbl_sims_fancy_ntests %>% knitr::kable("simple")
```




# References

- Reference1

- Reference2

```{r child = 'pss-cb-appendices.Rmd'}
```